<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BarkerMCMC · BarkerMCMC.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://scheidan.github.io/BarkerMCMC.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>BarkerMCMC.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>BarkerMCMC</a><ul class="internal"><li><a class="tocitem" href="#Usage"><span>Usage</span></a></li><li><a class="tocitem" href="#API"><span>API</span></a></li><li><a class="tocitem" href="#Related-Julia-Packages"><span>Related Julia Packages</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>BarkerMCMC</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>BarkerMCMC</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/scheidan/BarkerMCMC.jl/blob/master/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="BarkerMCMC.jl"><a class="docs-heading-anchor" href="#BarkerMCMC.jl">BarkerMCMC.jl</a><a id="BarkerMCMC.jl-1"></a><a class="docs-heading-anchor-permalink" href="#BarkerMCMC.jl" title="Permalink"></a></h1><p>Implements an adaptive Monte Carlo Markov Chain sampler that makes use of gradient information. It was the proposed by Livingstone et al. (2021).</p><p>The adaptative preconditioning is based on Andrieu and Thoms (2008), Algorithm 4 in Section 5. We followed the Algorithm 7.2 of the supporting information of Livingstone et al. (2021) with slight modifications.</p><p>You can find the repository with the source code <a href="https://github.com/scheidan/BarkerMCMC.jl">here</a>.</p><h2 id="Usage"><a class="docs-heading-anchor" href="#Usage">Usage</a><a id="Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Usage" title="Permalink"></a></h2><p>You can either define the log density compatible to <a href="https://github.com/tpapp/LogDensityProblems.jl">LogDensityProblems.jl</a>, or you provide the log density and it&#39;s gradient as two separate functions.</p><h3 id="LogDensityProblems-interface"><a class="docs-heading-anchor" href="#LogDensityProblems-interface"><code>LogDensityProblems</code> interface</a><a id="LogDensityProblems-interface-1"></a><a class="docs-heading-anchor-permalink" href="#LogDensityProblems-interface" title="Permalink"></a></h3><p>This approach is recommend to for most modeling tasks. This example demonstrate Bayesian inference of a simple regression. Note, that we use <code>TransformVariables.jl</code> to ensure that the standard deviation is always positive. The package takes care of the determinate correction.</p><pre><code class="nohighlight hljs">using BarkerMCMC

using TransformVariables: transform, inverse, as, asℝ, asℝ₊
using TransformedLogDensities: TransformedLogDensity
using LogDensityProblemsAD: ADgradient
using Distributions

# -----------
# simulate data

x = rand(10)
y = 2 .+ 1.5*x .+ randn(10)*0.2


# -----------
# define model

model(x, θ) = θ.a + θ.b*x

function likelihood(θ, x, y)
    ll = 0.0
    for i in eachindex(y)
        ll += logpdf(Normal(model(x[i], θ), θ.σ), y[i])
    end
    ll
end

function prior(θ)
    logpdf(Normal(0,1), θ.a) +
        logpdf(Normal(0,1), θ.b) +
        logpdf(Exponential(1), θ.σ)
end

posterior(θ, x, y) = likelihood(θ, x, y) + prior(θ)

# transformation σ to [0, ∞)
trans = as((a = asℝ, b = asℝ, σ = asℝ₊))

# -----------
# define lp

lp = TransformedLogDensity(trans, θ -&gt; posterior(θ, x, y))

# define gradient with AD
lp = ADgradient(:ForwardDiff, lp)
# lp= ADgradient(:Zygote, lp)  # we can use different AD backends


# -----------
# sample

# we need the inits in transformed space
inits = inverse(trans, (a=0, b=2, σ=0.5))

results = barker_mcmc(lp,
                      inits;
                      n_iter = 10_000)

# back-transform samples to original parameter space
samples = [transform(trans, s)
           for s in eachrow(results.samples)]

# convert to array
samplesArray = vcat((hcat(i...) for i in samples)...)</code></pre><p>See the example below how the results can be visualized.</p><h3 id="Function-interface"><a class="docs-heading-anchor" href="#Function-interface">Function interface</a><a id="Function-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Function-interface" title="Permalink"></a></h3><p>When not parameter transformations are required, the function interface can be a bit simpler to work with. Here we sample from the &#39;banana-shaped&#39; Rosenbruck function:</p><pre><code class="language-Julia hljs">using BarkerMCMC

# --- Define target distribution and it&#39;s gradient
#     (or use automatic differentation)

function log_p_rosebruck_2d(x; k=1/200)
    -k*(100*(x[2] - x[1]^2)^2 + (1 - x[1])^2)
end

function ∇log_p_rosebruck_2d(x; k=1/200)
    [-2*k*(200*x[1]^3 - 200*x[1]*x[2] + x[1] -1),   # d/dx[1]
     -200*k*(x[2] - x[1]^2)]                        # d/dx[2]
end

# --- Generate samples

res = barker_mcmc(log_p_rosebruck_2d,
                  ∇log_p_rosebruck_2d,
                  [5.0, 5.0];
                  n_iter = 1_000,
                  target_acceptance_rate=0.4)

res.samples
res.log_p

# --- Visualize results

# acceptance rate
length(unique(res.samples[:,1])) / size(res.samples, 1)

# You may want to use `MCMCChains.jl` for plots and diagonstics
# (must be installed separately)

using MCMCChains
using StatsPlots

chain = Chains(res.samples, [:x1, :x2])
chains[200:10:end]                 # remove burn-in and apply thinning

plot(chains)
meanplot(chain)
histogram(chain)
autocorplot(chain)
corner(chain)</code></pre><h2 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BarkerMCMC.barker_mcmc" href="#BarkerMCMC.barker_mcmc"><code>BarkerMCMC.barker_mcmc</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Adaptive MCMC sampler that makes use of gradient information. Based on Livingstone et al. (2020) The adaptation is based on Andrieu and Thoms (2008), Algorithm 4 in Section 5.</p><pre><code class="language-Julia hljs">barker_mcmc(lp,
            inits::AbstractVector;
            n_iter = 100::Int,
            σ = 2.4/(length(inits)^(1/6)),
            target_acceptance_rate = 0.4,
            κ::Float64 = 0.6,
            n_iter_adaptation = Inf,
            preconditioning::Function = BarkerMCMC.precond_eigen)</code></pre><p>or</p><pre><code class="language-Julia hljs">barker_mcmc(log_p::Function, ∇log_p::Function,
            inits::AbstractVector;
            n_iter = 100::Int,
            σ = 2.4/(length(inits)^(1/6)),
            target_acceptance_rate = 0.4,
            κ::Float64 = 0.6,
            n_iter_adaptation = Inf,
            preconditioning::Function = BarkerMCMC.precond_eigen)</code></pre><p><strong>Arguments</strong></p><ul><li><code>lp</code>: log density object that supports the API of the <code>LogDensityProblems.jl</code> package</li><li><code>log_p::Function</code>: function returning the log of the (non-normalized) density</li><li><code>∇log_p::Function</code>: function returning the gradient of log of the (non-normalized) density</li><li><code>inits::Vector</code>: initial starting values</li><li><code>n_iter = 100</code>: number of iterations</li><li><code>σ = 2.4/(length(inits)^(1/6))</code>: global scale of proposal distribution</li><li><code>target_acceptance_rate = 0.4</code>: desired acceptance rate</li><li><code>κ = 0.6</code>: controls adaptation speed, κ ∈ (0.5, 1). Larger values lead to slower adaptation, see Section 6.1            in Livingstone et al. (2020).</li><li><code>n_iter_adaptation = Inf</code>: number of iterations with adaptation</li><li><code>preconditioning::Function = BarkerMCMC.precond_eigen</code>: Either <code>BarkerMCMC.precond_eigen</code> or <code>BarkerMCMC.precond_cholesky</code>. Calculating the preconditioning matrix with a cholesky decomposition is slighly cheaper, however, the eigen value decomposition allows for a proper rotation of the proposal distribution.</li></ul><p><strong>Return Value</strong></p><p>A named tuple with fields:</p><ul><li><code>samples</code>: array containing the samples</li><li><code>log_p</code>: vector containing the value of <code>log_p</code> for each sample.</li></ul><p><strong>References</strong></p><p>Andrieu, C., Thoms, J., 2008. A tutorial on adaptive MCMC. Statistics and computing 18, 343–373.</p><p>Livingstone, S., Zanella, G., 2021. The Barker proposal: Combining robustness and efficiency in gradient-based MCMC. Journal of the Royal Statistical Society: Series B (Statistical Methodology). https://doi.org/10.1111/rssb.12482</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scheidan/BarkerMCMC.jl/blob/f15ee23cdc2605406469863eb80f383cc0ba9395/src/BarkerMCMC.jl#L12-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BarkerMCMC.precond_cholesky" href="#BarkerMCMC.precond_cholesky"><code>BarkerMCMC.precond_cholesky</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Given a covariance matrix Σ, computes the preconditioning matrix <code>M</code> based on cholesky decomposition.</p><p>For <code>M</code> holds that cov(M * z) == Σ, where <code>z</code> a uncorrelated vector of random variables with zero mean.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scheidan/BarkerMCMC.jl/blob/f15ee23cdc2605406469863eb80f383cc0ba9395/src/BarkerMCMC.jl#L191-L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BarkerMCMC.precond_eigen" href="#BarkerMCMC.precond_eigen"><code>BarkerMCMC.precond_eigen</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Given a covariance matrix Σ, computes the preconditioning matrix <code>M</code> based on eigen value decomposition.</p><p>For <code>M</code> holds that cov(M * z) == Σ, where <code>z</code> a uncorrelated vector of random variables with zero mean.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scheidan/BarkerMCMC.jl/blob/f15ee23cdc2605406469863eb80f383cc0ba9395/src/BarkerMCMC.jl#L200-L206">source</a></section></article><h2 id="Related-Julia-Packages"><a class="docs-heading-anchor" href="#Related-Julia-Packages">Related Julia Packages</a><a id="Related-Julia-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Related-Julia-Packages" title="Permalink"></a></h2><ul><li><a href="https://github.com/tpapp/LogDensityProblems.jl">LogDensityProblems.jl</a></li><li><a href="https://github.com/tpapp/TransformVariables.jl">TransformVariables.jl</a></li><li><a href="https://github.com/TuringLang/MCMCChains.jl">MCMCChains.jl</a></li></ul><h4 id="Hamiltonian-Monte-Carlo-(gradient-based)"><a class="docs-heading-anchor" href="#Hamiltonian-Monte-Carlo-(gradient-based)">Hamiltonian Monte Carlo (gradient based)</a><a id="Hamiltonian-Monte-Carlo-(gradient-based)-1"></a><a class="docs-heading-anchor-permalink" href="#Hamiltonian-Monte-Carlo-(gradient-based)" title="Permalink"></a></h4><ul><li><a href="https://github.com/tpapp/DynamicHMC.jl">DynamicHMC.jl</a></li><li><a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a></li></ul><h4 id="Adaptive-MCMC-(without-gradient)"><a class="docs-heading-anchor" href="#Adaptive-MCMC-(without-gradient)">Adaptive MCMC (without gradient)</a><a id="Adaptive-MCMC-(without-gradient)-1"></a><a class="docs-heading-anchor-permalink" href="#Adaptive-MCMC-(without-gradient)" title="Permalink"></a></h4><ul><li><a href="https://github.com/mvihola/AdaptiveMCMC.jl">AdaptiveMCMC.jl</a></li><li><a href="https://github.com/anthofflab/RobustAdaptiveMetropolisSampler.jl">RobustAdaptiveMetropolisSampler.jl</a></li><li><a href="https://github.com/mauro3/KissMCMC.jl">KissMCMC.jl</a></li></ul><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><p>Andrieu, C., Thoms, J., 2008. A tutorial on adaptive MCMC. Statistics and computing 18, 343–373.</p><p>Livingstone, S., Zanella, G., 2021. The Barker proposal: Combining robustness and efficiency in gradient-based MCMC. Journal of the Royal Statistical Society: Series B (Statistical Methodology). <a href="https://doi.org/10.1111/rssb.12482">https://doi.org/10.1111/rssb.12482</a></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.16 on <span class="colophon-date" title="Friday 17 May 2024 15:11">Friday 17 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
